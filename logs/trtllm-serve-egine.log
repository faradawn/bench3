/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[TensorRT-LLM] TensorRT LLM version: 1.2.0rc4
/usr/local/lib/python3.12/dist-packages/tensorrt_llm/serve/openai_protocol.py:94: UserWarning: Field name "schema" in "ResponseFormat" shadows an attribute in parent "OpenAIBaseModel"
  class ResponseFormat(OpenAIBaseModel):
[12/07/2025-00:20:15] [TRT-LLM] [I] Using LLM with PyTorch backend
[12/07/2025-00:20:15] [TRT-LLM] [I] Set PluginConfig.nccl_plugin to None.
[12/07/2025-00:20:15] [TRT-LLM] [I] neither checkpoint_format nor checkpoint_loader were provided, checkpoint_format will be set to HF.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
You are using a model of type llama to instantiate a model of type . This is not supported for all configurations of models and can yield errors.
`torch_dtype` is deprecated! Use `dtype` instead!
[12/07/2025-00:20:16] [TRT-LLM] [W] Orchestrator is creating IPC executor
[33;20mrank 0 using MpiPoolSession to spawn MPI processes
[0m[12/07/2025-00:20:16] [TRT-LLM] [I] Generating a new HMAC key for server proxy_request_queue
[12/07/2025-00:20:16] [TRT-LLM] [I] Generating a new HMAC key for server worker_init_status_queue
[12/07/2025-00:20:16] [TRT-LLM] [I] Generating a new HMAC key for server proxy_result_queue
[12/07/2025-00:20:16] [TRT-LLM] [I] Generating a new HMAC key for server proxy_stats_queue
[12/07/2025-00:20:16] [TRT-LLM] [I] Generating a new HMAC key for server proxy_kv_cache_events_queue
[1765066816.574825] [d0c32c26d329:331  :0]     ucp_context.c:2339 UCX  WARN  UCP API version is incompatible: required >= 1.20, actual 1.19.0 (loaded from /usr/local/ucx//lib/libucp.so.0)
[1765066816.883560] [d0c32c26d329:331  :0]     ucp_context.c:2339 UCX  WARN  UCP API version is incompatible: required >= 1.20, actual 1.19.0 (loaded from /usr/local/ucx//lib/libucp.so.0)
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Multiple distributions found for package optimum. Picked distribution: optimum
[TensorRT-LLM] TensorRT LLM version: 1.2.0rc4
/usr/local/lib/python3.12/dist-packages/tensorrt_llm/serve/openai_protocol.py:94: UserWarning: Field name "schema" in "ResponseFormat" shadows an attribute in parent "OpenAIBaseModel"
  class ResponseFormat(OpenAIBaseModel):
[TensorRT-LLM][INFO] Refreshed the MPI local session
[12/07/2025-00:20:27] [TRT-LLM] [W] Worker process 331 is affined to run on the following CPUs: [2, 146] (subset of all logical CPUs). This may harm performance if set incorrectly.
[12/07/2025-00:20:27] [TRT-LLM] [I] ATTENTION RUNTIME FEATURES:  AttentionRuntimeFeatures(chunked_prefill=False, cache_reuse=True, has_speculative_draft_tokens=False, chunk_size=1024, chunked_prefill_buffer_batch_size=4)
`torch_dtype` is deprecated! Use `dtype` instead!
[12/07/2025-00:20:27] [TRT-LLM] [I] Validating KV Cache config against kv_cache_dtype="auto"
[12/07/2025-00:20:27] [TRT-LLM] [I] KV cache quantization set to "auto". Using checkpoint KV quantization.
[12/07/2025-00:20:27] [TRT-LLM] [I] Use 14.96 GB for model weights.
[12/07/2025-00:20:27] [TRT-LLM] [I] Prefetching 14.96GB checkpoint files.
[12/07/2025-00:20:27] [TRT-LLM] [I] Prefetching /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/model-00002-of-00004.safetensors to memory...
[12/07/2025-00:20:27] [TRT-LLM] [I] Prefetching /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/model-00001-of-00004.safetensors to memory...
[12/07/2025-00:20:27] [TRT-LLM] [I] Prefetching /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/model-00004-of-00004.safetensors to memory...
[12/07/2025-00:20:27] [TRT-LLM] [I] Prefetching /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/model-00003-of-00004.safetensors to memory...
[12/07/2025-00:20:29] [TRT-LLM] [I] Finished prefetching /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/model-00004-of-00004.safetensors.
[12/07/2025-00:20:39] [TRT-LLM] [I] Finished prefetching /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/model-00001-of-00004.safetensors.
[12/07/2025-00:20:40] [TRT-LLM] [I] Finished prefetching /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/model-00003-of-00004.safetensors.
[12/07/2025-00:20:40] [TRT-LLM] [I] Finished prefetching /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/model-00002-of-00004.safetensors.
Loading safetensors weights in parallel:   0%|          | 0/4 [00:00<?, ?it/s][12/07/2025-00:20:40] [TRT-LLM] [I] Start to load safetensor file /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/model-00002-of-00004.safetensors
[12/07/2025-00:20:40] [TRT-LLM] [I] Start to load safetensor file /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/model-00004-of-00004.safetensors
[12/07/2025-00:20:40] [TRT-LLM] [I] Start to load safetensor file /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/model-00001-of-00004.safetensors
[12/07/2025-00:20:40] [TRT-LLM] [I] Start to load safetensor file /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/model-00003-of-00004.safetensors
Loading safetensors weights in parallel: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 55.00it/s]
Loading weights:   0%|          | 0/681 [00:00<?, ?it/s]Loading weights:   0%|          | 3/681 [00:00<00:48, 13.99it/s]Loading weights:   6%|â–Œ         | 38/681 [00:00<00:04, 139.89it/s]Loading weights:  12%|â–ˆâ–        | 80/681 [00:00<00:02, 216.37it/s]Loading weights:  18%|â–ˆâ–Š        | 122/681 [00:00<00:02, 257.63it/s]Loading weights:  24%|â–ˆâ–ˆâ–       | 164/681 [00:00<00:01, 282.07it/s]Loading weights:  30%|â–ˆâ–ˆâ–ˆ       | 206/681 [00:00<00:01, 298.03it/s]Loading weights:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 248/681 [00:00<00:01, 306.24it/s]Loading weights:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 290/681 [00:01<00:01, 313.80it/s]Loading weights:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 332/681 [00:01<00:01, 318.24it/s]Loading weights:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 374/681 [00:01<00:00, 321.38it/s]Loading weights:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 416/681 [00:01<00:00, 323.85it/s]Loading weights:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 458/681 [00:01<00:00, 325.43it/s]Loading weights:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 500/681 [00:01<00:00, 325.04it/s]Loading weights:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 542/681 [00:01<00:00, 327.09it/s]Loading weights:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 578/681 [00:01<00:00, 335.39it/s]Loading weights:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 612/681 [00:02<00:00, 322.11it/s]Loading weights:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 647/681 [00:02<00:00, 320.47it/s]Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 680/681 [00:02<00:00, 235.69it/s]Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 681/681 [00:02<00:00, 280.27it/s]
Model init total -- 15.29s
[12/07/2025-00:20:42] [TRT-LLM] [I] Using Sampler: TorchSampler
[12/07/2025-00:20:42] [TRT-LLM] [W] Both free_gpu_memory_fraction and max_tokens are set (to 0.9 and 1088 with free memory 122.65985107421875GiB of total memory 139.80084228515625GiB, respectively). The smaller value will be used.
[TensorRT-LLM][INFO] Max KV cache blocks per sequence: 33 [window size=1025], tokens per block=32, primary blocks=34, secondary blocks=0, max sequence length=1025
[TensorRT-LLM][INFO] Number of tokens per block: 32.
[TensorRT-LLM][INFO] [MemUsageChange] Allocated 0.13 GiB for max tokens in paged KV cache (1088).
[12/07/2025-00:20:42] [TRT-LLM] [I] max_seq_len=1025, max_num_requests=32, max_num_tokens=1024, max_batch_size=32
[12/07/2025-00:20:42] [TRT-LLM] [I] cache_transceiver is disabled
[12/07/2025-00:20:42] [TRT-LLM] [I] Running autotuner warmup...
[12/07/2025-00:20:42] [TRT-LLM] [I] [Autotuner] Autotuning process starts ...
[TensorRT-LLM][WARNING] Attention workspace size is not enough, increase the size from 0 bytes to 41960448 bytes
[12/07/2025-00:20:58] [TRT-LLM] [I] [Autotuner] Autotuning process ends
[12/07/2025-00:20:58] [TRT-LLM] [I] [Autotuner] Cache size after warmup is 0
[12/07/2025-00:20:58] [TRT-LLM] [I] Creating CUDA graph instances for 32 batch sizes.
[12/07/2025-00:20:58] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=32, draft_len=0
[12/07/2025-00:20:59] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=31, draft_len=0
[12/07/2025-00:20:59] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=30, draft_len=0
[12/07/2025-00:20:59] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=29, draft_len=0
[12/07/2025-00:20:59] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=28, draft_len=0
[12/07/2025-00:20:59] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=27, draft_len=0
[12/07/2025-00:20:59] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=26, draft_len=0
[12/07/2025-00:20:59] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=25, draft_len=0
[12/07/2025-00:20:59] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=24, draft_len=0
[12/07/2025-00:20:59] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=23, draft_len=0
[12/07/2025-00:20:59] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=22, draft_len=0
[12/07/2025-00:20:59] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=21, draft_len=0
[12/07/2025-00:20:59] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=20, draft_len=0
[12/07/2025-00:20:59] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=19, draft_len=0
[12/07/2025-00:20:59] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=18, draft_len=0
[12/07/2025-00:20:59] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=17, draft_len=0
[12/07/2025-00:20:59] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=16, draft_len=0
[12/07/2025-00:20:59] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=15, draft_len=0
[12/07/2025-00:20:59] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=14, draft_len=0
[12/07/2025-00:20:59] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=13, draft_len=0
[12/07/2025-00:20:59] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=12, draft_len=0
[12/07/2025-00:20:59] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=11, draft_len=0
[12/07/2025-00:20:59] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=10, draft_len=0
[12/07/2025-00:20:59] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=9, draft_len=0
[12/07/2025-00:20:59] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=8, draft_len=0
[12/07/2025-00:21:00] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=7, draft_len=0
[12/07/2025-00:21:00] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=6, draft_len=0
[12/07/2025-00:21:00] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=5, draft_len=0
[12/07/2025-00:21:00] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=4, draft_len=0
[12/07/2025-00:21:00] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=3, draft_len=0
[12/07/2025-00:21:00] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=2, draft_len=0
[12/07/2025-00:21:00] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=1, draft_len=0
[12/07/2025-00:21:00] [TRT-LLM] [I] global_steady_clock_offset at each rank: [0.0]
[12/07/2025-00:21:00] [TRT-LLM] [I] Setting global_steady_clock_offset: 0.0 seconds for rank 0
[12/07/2025-00:21:00] [TRT-LLM] [I] Memory used after loading model weights (inside torch) in memory usage profiling: 15.06 GiB
[12/07/2025-00:21:00] [TRT-LLM] [I] Memory used after loading model weights (outside torch) in memory usage profiling: 2.52 GiB
[TensorRT-LLM][WARNING] [kv cache manager] storeContextBlocks: Can not find sequence for request 32
[TensorRT-LLM][WARNING] [kv cache manager] storeContextBlocks: Can not find sequence for request 33
[12/07/2025-00:21:00] [TRT-LLM] [I] Memory dynamically allocated during inference (inside torch) in memory usage profiling: 0.14 GiB
[12/07/2025-00:21:00] [TRT-LLM] [I] Memory used outside torch (e.g., NCCL and CUDA graphs) in memory usage profiling: 2.55 GiB
[12/07/2025-00:21:00] [TRT-LLM] [I] Peak memory during memory usage profiling (torch + non-torch): 17.76 GiB, available KV cache memory when calculating max tokens: 109.96 GiB, fraction is set 0.9, kv size is 131072. device total memory 139.80 GiB, , tmp kv_mem 0.13 GiB
[12/07/2025-00:21:00] [TRT-LLM] [I] Estimated max memory in KV cache : 109.96 GiB
[12/07/2025-00:21:00] [TRT-LLM] [W] Both free_gpu_memory_fraction and max_tokens are set (to 0.9 and 900796 with free memory 122.29071044921875GiB of total memory 139.80084228515625GiB, respectively). The smaller value will be used.
[TensorRT-LLM][INFO] Max KV cache blocks per sequence: 33 [window size=1025], tokens per block=32, primary blocks=28150, secondary blocks=0, max sequence length=1025
[TensorRT-LLM][INFO] Number of tokens per block: 32.
[TensorRT-LLM][INFO] [MemUsageChange] Allocated 109.96 GiB for max tokens in paged KV cache (900800).
[12/07/2025-00:21:00] [TRT-LLM] [I] max_seq_len=1025, max_num_requests=32, max_num_tokens=1024, max_batch_size=32
[12/07/2025-00:21:00] [TRT-LLM] [I] cache_transceiver is disabled
[12/07/2025-00:21:00] [TRT-LLM] [I] Running autotuner warmup...
[12/07/2025-00:21:00] [TRT-LLM] [I] [Autotuner] Autotuning process starts ...
[TensorRT-LLM][WARNING] Attention workspace size is not enough, increase the size from 0 bytes to 41960448 bytes
[12/07/2025-00:21:00] [TRT-LLM] [I] [Autotuner] Autotuning process ends
[12/07/2025-00:21:00] [TRT-LLM] [I] [Autotuner] Cache size after warmup is 0
[12/07/2025-00:21:00] [TRT-LLM] [I] Creating CUDA graph instances for 32 batch sizes.
[12/07/2025-00:21:00] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=32, draft_len=0
[12/07/2025-00:21:00] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=31, draft_len=0
[12/07/2025-00:21:00] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=30, draft_len=0
[12/07/2025-00:21:00] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=29, draft_len=0
[12/07/2025-00:21:00] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=28, draft_len=0
[12/07/2025-00:21:00] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=27, draft_len=0
[12/07/2025-00:21:00] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=26, draft_len=0
[12/07/2025-00:21:00] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=25, draft_len=0
[12/07/2025-00:21:00] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=24, draft_len=0
[12/07/2025-00:21:00] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=23, draft_len=0
[12/07/2025-00:21:00] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=22, draft_len=0
[12/07/2025-00:21:01] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=21, draft_len=0
[12/07/2025-00:21:01] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=20, draft_len=0
[12/07/2025-00:21:01] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=19, draft_len=0
[12/07/2025-00:21:01] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=18, draft_len=0
[12/07/2025-00:21:01] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=17, draft_len=0
[12/07/2025-00:21:01] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=16, draft_len=0
[12/07/2025-00:21:01] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=15, draft_len=0
[12/07/2025-00:21:01] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=14, draft_len=0
[12/07/2025-00:21:01] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=13, draft_len=0
[12/07/2025-00:21:01] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=12, draft_len=0
[12/07/2025-00:21:01] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=11, draft_len=0
[12/07/2025-00:21:01] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=10, draft_len=0
[12/07/2025-00:21:01] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=9, draft_len=0
[12/07/2025-00:21:01] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=8, draft_len=0
[12/07/2025-00:21:01] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=7, draft_len=0
[12/07/2025-00:21:01] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=6, draft_len=0
[12/07/2025-00:21:01] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=5, draft_len=0
[12/07/2025-00:21:01] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=4, draft_len=0
[12/07/2025-00:21:01] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=3, draft_len=0
[12/07/2025-00:21:01] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=2, draft_len=0
[12/07/2025-00:21:01] [TRT-LLM] [I] Run generation-only CUDA graph warmup for batch size=1, draft_len=0
[12/07/2025-00:21:01] [TRT-LLM] [I] global_steady_clock_offset at each rank: [0.0]
[12/07/2025-00:21:01] [TRT-LLM] [I] Setting global_steady_clock_offset: 0.0 seconds for rank 0
[12/07/2025-00:21:01] [TRT-LLM] [I] Setting PyTorch memory fraction to 0.20262982226128215 (28.32781982421875 GiB)
[12/07/2025-00:21:01] [TRT-LLM] [I] LLM Args:
model='meta-llama/Meta-Llama-3-8B-Instruct' tokenizer=None tokenizer_mode='auto' skip_tokenizer_init=False trust_remote_code=False tensor_parallel_size=1 dtype='auto' revision=None tokenizer_revision=None pipeline_parallel_size=1 context_parallel_size=1 gpus_per_node=2 moe_cluster_parallel_size=-1 moe_tensor_parallel_size=-1 moe_expert_parallel_size=-1 enable_attention_dp=False enable_lm_head_tp_in_adp=False pp_partition=None cp_config={} load_format=<LoadFormat.AUTO: 0> fail_fast_on_attention_window_too_large=False enable_lora=False lora_config=None kv_cache_config=KvCacheConfig(enable_block_reuse=True, max_tokens=900796, max_attention_window=None, sink_token_length=None, free_gpu_memory_fraction=0.9, host_cache_size=None, onboard_blocks=True, cross_kv_cache_fraction=None, secondary_offload_min_priority=None, event_buffer_max_size=0, attention_dp_events_gather_period_ms=5, enable_partial_reuse=True, copy_on_partial_reuse=True, use_uvm=False, max_gpu_total_bytes=118069237555, dtype='auto', mamba_ssm_cache_dtype='auto', tokens_per_block=32) enable_chunked_prefill=False guided_decoding_backend=None batched_logits_processor=None iter_stats_max_iterations=None request_stats_max_iterations=None peft_cache_config=None scheduler_config=SchedulerConfig(capacity_scheduler_policy=<CapacitySchedulerPolicy.GUARANTEED_NO_EVICT: 'GUARANTEED_NO_EVICT'>, context_chunking_policy=None, dynamic_batch_config=DynamicBatchConfig(enable_batch_size_tuning=True, enable_max_num_tokens_tuning=False, dynamic_batch_moving_average_window=128)) cache_transceiver_config=None sparse_attention_config=None speculative_config=None max_batch_size=32 max_input_len=1024 max_seq_len=1024 max_beam_width=1 max_num_tokens=1024 gather_generation_logits=False num_postprocess_workers=0 postprocess_tokenizer_dir='meta-llama/Meta-Llama-3-8B-Instruct' reasoning_parser=None decoding_config=None mpi_session=None otlp_traces_endpoint=None backend='pytorch' return_perf_metrics=False orchestrator_type=None build_config=BuildConfig(max_input_len=1024, max_seq_len=1024, opt_batch_size=8, max_batch_size=32, max_beam_width=1, max_num_tokens=1024, opt_num_tokens=None, max_prompt_embedding_table_size=0, kv_cache_type=None, gather_context_logits=False, gather_generation_logits=False, strongly_typed=True, force_num_profiles=None, profiling_verbosity='layer_names_only', enable_debug_output=False, max_draft_len=0, speculative_decoding_mode=<SpeculativeDecodingMode.NONE: 1>, use_refit=False, input_timing_cache=None, output_timing_cache='model.cache', lora_config=LoraConfig(lora_dir=[], lora_ckpt_source='hf', max_lora_rank=64, lora_target_modules=[], trtllm_modules_to_hf_modules={}, max_loras=None, max_cpu_loras=None, swap_gate_up_proj_lora_b_weight=True), weight_sparsity=False, weight_streaming=False, plugin_config=PluginConfig(dtype='float16', bert_attention_plugin='auto', gpt_attention_plugin='auto', gemm_plugin=None, gemm_swiglu_plugin=None, fp8_rowwise_gemm_plugin=None, qserve_gemm_plugin=None, identity_plugin=None, nccl_plugin=None, lora_plugin=None, dora_plugin=False, weight_only_groupwise_quant_matmul_plugin=None, weight_only_quant_matmul_plugin=None, smooth_quant_plugins=True, smooth_quant_gemm_plugin=None, layernorm_quantization_plugin=None, rmsnorm_quantization_plugin=None, quantize_per_token_plugin=False, quantize_tensor_plugin=False, moe_plugin='auto', mamba_conv1d_plugin='auto', low_latency_gemm_plugin=None, low_latency_gemm_swiglu_plugin=None, gemm_allreduce_plugin=None, context_fmha=True, bert_context_fmha_fp32_acc=False, paged_kv_cache=None, remove_input_padding=True, norm_quant_fusion=False, reduce_fusion=False, user_buffer=False, tokens_per_block=32, use_paged_context_fmha=True, use_fp8_context_fmha=True, fuse_fp4_quant=False, multiple_profiles=False, paged_state=True, streamingllm=False, manage_weights=False, use_fused_mlp=True, pp_reduce_scatter=False), use_strip_plan=False, max_encoder_input_len=1024, dry_run=False, visualize_network=None, monitor_memory=False, use_mrope=False) garbage_collection_gen0_threshold=20000 cuda_graph_config=CudaGraphConfig(batch_sizes=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 64, 128], max_batch_size=128, enable_padding=False) attention_dp_config=None disable_overlap_scheduler=False moe_config=MoeConfig(backend='CUTLASS', max_num_tokens=None, load_balancer=None, disable_finalize_fusion=False, use_low_precision_moe_combine=False) attn_backend='TRTLLM' sampler_type=<SamplerType.auto: 'auto'> enable_iter_perf_stats=False enable_iter_req_stats=False print_iter_log=False perf_metrics_max_requests=0 batch_wait_timeout_ms=0 batch_wait_timeout_iters=0 batch_wait_max_tokens_ratio=0 torch_compile_config=None enable_autotuner=True enable_layerwise_nvtx_marker=False enable_min_latency=False stream_interval=1 force_dynamic_quantization=False allreduce_strategy='AUTO' checkpoint_loader=None checkpoint_format='HF' kv_connector_config=None mm_encoder_only=False ray_worker_extension_cls=None enable_sleep=False
[12/07/2025-00:21:01] [TRT-LLM] [I] get signal from executor worker
INFO:     Started server process [145]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
INFO:     172.17.0.1:42698 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     172.17.0.1:42710 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     172.17.0.1:42716 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     172.17.0.1:46774 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     172.17.0.1:46790 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     172.17.0.1:46796 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     172.17.0.1:45246 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     172.17.0.1:45258 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     172.17.0.1:45270 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     172.17.0.1:56500 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     172.17.0.1:56516 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     172.17.0.1:56532 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     172.17.0.1:42702 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     172.17.0.1:42722 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     172.17.0.1:42712 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     172.17.0.1:42738 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     172.17.0.1:42732 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     172.17.0.1:42750 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     172.17.0.1:57150 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     172.17.0.1:58224 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     172.17.0.1:58230 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     172.17.0.1:58274 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     172.17.0.1:58246 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     172.17.0.1:58258 - "POST /v1/completions HTTP/1.1" 200 OK
